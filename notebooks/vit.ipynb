{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b14f00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\ernan\\project\\zeromodel\\venv\\lib\\site-packages (2.8.0+cu126)\n",
      "Requirement already satisfied: torchvision in c:\\users\\ernan\\project\\zeromodel\\venv\\lib\\site-packages (0.23.0+cu126)\n",
      "Requirement already satisfied: timm in c:\\users\\ernan\\project\\zeromodel\\venv\\lib\\site-packages (1.0.19)\n",
      "Requirement already satisfied: pillow in c:\\users\\ernan\\project\\zeromodel\\venv\\lib\\site-packages (11.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ernan\\project\\zeromodel\\venv\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\ernan\\project\\zeromodel\\venv\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ernan\\project\\zeromodel\\venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ernan\\project\\zeromodel\\venv\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\ernan\\project\\zeromodel\\venv\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ernan\\project\\zeromodel\\venv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ernan\\project\\zeromodel\\venv\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\ernan\\project\\zeromodel\\venv\\lib\\site-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\ernan\\project\\zeromodel\\venv\\lib\\site-packages (from timm) (0.34.4)\n",
      "Requirement already satisfied: safetensors in c:\\users\\ernan\\project\\zeromodel\\venv\\lib\\site-packages (from timm) (0.6.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ernan\\project\\zeromodel\\venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\ernan\\project\\zeromodel\\venv\\lib\\site-packages (from huggingface_hub->timm) (25.0)\n",
      "Requirement already satisfied: requests in c:\\users\\ernan\\project\\zeromodel\\venv\\lib\\site-packages (from huggingface_hub->timm) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\ernan\\project\\zeromodel\\venv\\lib\\site-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ernan\\project\\zeromodel\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub->timm) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ernan\\project\\zeromodel\\venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ernan\\project\\zeromodel\\venv\\lib\\site-packages (from requests->huggingface_hub->timm) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ernan\\project\\zeromodel\\venv\\lib\\site-packages (from requests->huggingface_hub->timm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ernan\\project\\zeromodel\\venv\\lib\\site-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ernan\\project\\zeromodel\\venv\\lib\\site-packages (from requests->huggingface_hub->timm) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision timm pillow numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "187930e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0acc0ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "from pathlib import Path\n",
    "\n",
    "# Get the absolute path to the base directory (one level up from notebook dir)\n",
    "BASE_DIR = Path(__file__).resolve().parent.parent if \"__file__\" in globals() else Path.cwd().parent\n",
    "\n",
    "# Add to sys.path if not already present\n",
    "if str(BASE_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(BASE_DIR))\n",
    "\n",
    "# Now you can import directly\n",
    "from zeromodel import ZeroModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a831186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, time, functools, numpy as np\n",
    "from collections import deque\n",
    "from PIL import Image, ImageDraw\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as dsets\n",
    "import timm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1873035c",
   "metadata": {},
   "source": [
    "## CIFAR-10 dataloaders (fixed eval batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbe9cf7",
   "metadata": {},
   "source": [
    "## ViT model + hooks to capture attention & token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b478151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook_emitter.py\n",
    "import os, json, time, numpy as np\n",
    "\n",
    "class ZMFileEmitter:\n",
    "    def __init__(self, run_dir=\"artifacts/zm_run1\", fps=8):\n",
    "        self.run_dir = run_dir\n",
    "        os.makedirs(f\"{run_dir}/frames\", exist_ok=True)\n",
    "        self.meta_f = open(f\"{run_dir}/meta.jsonl\", \"a\", buffering=1)\n",
    "        # announce run (ZeroModel tailer watches for run.json)\n",
    "        with open(f\"{run_dir}/run.json\",\"w\") as f:\n",
    "            json.dump({\"status\":\"open\",\"fps\":fps}, f)\n",
    "\n",
    "    def send(self, step:int, frame_hwC:np.ndarray, tags:dict=None):\n",
    "        # frame: float32 [H,W,C] in [0,1]\n",
    "        path = f\"{self.run_dir}/frames/{step:06d}.npy\"\n",
    "        np.save(path, frame_hwC.astype(\"float32\"))\n",
    "        rec = {\"step\": step, \"path\": path, \"tags\": tags or {}, \"ts\": time.time()}\n",
    "        self.meta_f.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "    def close(self):\n",
    "        self.meta_f.flush(); self.meta_f.close()\n",
    "        with open(f\"{self.run_dir}/run.json\",\"w\") as f:\n",
    "            json.dump({\"status\":\"closed\"}, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98a20444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 0) Imports & Utilities\n",
    "# =========================\n",
    "import os, json, math, time, numpy as np\n",
    "from collections import deque\n",
    "from contextlib import contextmanager\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as dsets\n",
    "import timm\n",
    "from timm.models.vision_transformer import Attention\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# --- Simple local GIF logger (optional preview) ---\n",
    "class GifLogger:\n",
    "    def __init__(self, path=\"cifar_vit.gif\", fps=8):\n",
    "        self.frames=[]; self.path=path; self.duration=int(1000/fps)\n",
    "    def add(self, pil_img, text=None):\n",
    "        im = pil_img.convert(\"P\", palette=Image.ADAPTIVE, colors=256)\n",
    "        if text:\n",
    "            draw = ImageDraw.Draw(im); draw.text((4,4), text, fill=255)\n",
    "        self.frames.append(im)\n",
    "    def save(self):\n",
    "        if self.frames:\n",
    "            os.makedirs(os.path.dirname(self.path) or \".\", exist_ok=True)\n",
    "            self.frames[0].save(self.path, save_all=True, append_images=self.frames[1:],\n",
    "                                optimize=True, duration=self.duration, loop=0)\n",
    "\n",
    "# --- ZeroModel file-drop emitter (frames + metadata) ---\n",
    "class ZMFileEmitter:\n",
    "    def __init__(self, run_dir=\"artifacts/zm_run1\", fps=8):\n",
    "        self.run_dir = run_dir\n",
    "        os.makedirs(f\"{run_dir}/frames\", exist_ok=True)\n",
    "        self.meta_path = f\"{run_dir}/meta.jsonl\"\n",
    "        self.meta_f = open(self.meta_path, \"a\", buffering=1)\n",
    "        with open(f\"{run_dir}/run.json\",\"w\") as f:\n",
    "            json.dump({\"status\":\"open\",\"fps\":fps}, f)\n",
    "    def send(self, step:int, frame_hwC:np.ndarray, tags:dict=None):\n",
    "        path = f\"{self.run_dir}/frames/{step:06d}.npy\"\n",
    "        np.save(path, frame_hwC.astype(\"float32\"))\n",
    "        rec = {\"step\": step, \"path\": path, \"tags\": tags or {}, \"ts\": time.time()}\n",
    "        self.meta_f.write(json.dumps(rec) + \"\\n\")\n",
    "    def close(self):\n",
    "        self.meta_f.flush(); self.meta_f.close()\n",
    "        with open(f\"{self.run_dir}/run.json\",\"w\") as f:\n",
    "            json.dump({\"status\":\"closed\"}, f)\n",
    "\n",
    "# --- image helper ---\n",
    "def to_img(m, H=128, W=128):\n",
    "    m = m.astype(np.float32)\n",
    "    m = (m - m.min()) / (m.max() - m.min() + 1e-8)\n",
    "    im = Image.fromarray((m*255).astype(np.uint8))\n",
    "    return im.resize((W, H), resample=Image.BILINEAR)\n",
    "\n",
    "def tile(imgs, cols=3):\n",
    "    w, h = imgs[0].size\n",
    "    rows = (len(imgs)+cols-1)//cols\n",
    "    canvas = Image.new(\"L\", (cols*w, rows*h))\n",
    "    for i, im in enumerate(imgs):\n",
    "        r, c = divmod(i, cols)\n",
    "        canvas.paste(im, (c*w, r*h))\n",
    "    return canvas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d43460d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_gram(tokens):  # tokens: (B,N,C)\n",
    "    # use fixed eval batch → take mean over batch\n",
    "    t = tokens.mean(dim=0)         # (N,C)\n",
    "    t = F.normalize(t, dim=1)\n",
    "    G = t @ t.T                    # (N,N), values in [-1,1]\n",
    "    return G\n",
    "\n",
    "def to_img(m, H=128, W=128):\n",
    "    m = m.astype(np.float32)\n",
    "    m = (m - m.min()) / (m.max() - m.min() + 1e-8)\n",
    "    im = Image.fromarray((m*255).astype(np.uint8))\n",
    "    return im.resize((W, H), resample=Image.BILINEAR)\n",
    "\n",
    "def tile(imgs, cols=2):\n",
    "    if not imgs: return Image.new(\"L\",(1,1))\n",
    "    w,h = imgs[0].size\n",
    "    rows = (len(imgs)+cols-1)//cols\n",
    "    canvas = Image.new(\"L\",(cols*w, rows*h))\n",
    "    for i,im in enumerate(imgs):\n",
    "        r,c = divmod(i,cols)\n",
    "        canvas.paste(im,(c*w, r*h))\n",
    "    return canvas\n",
    "\n",
    "class GifLogger:\n",
    "    def __init__(self, path=\"cifar_vit.gif\", fps=8):\n",
    "        self.frames=[]; self.path=path; self.duration=int(1000/fps)\n",
    "    def add(self, pil_img, text=None):\n",
    "        im = pil_img.convert(\"P\", palette=Image.ADAPTIVE, colors=256)\n",
    "        if text:\n",
    "            draw = ImageDraw.Draw(im); draw.text((4,4), text, fill=255)\n",
    "        self.frames.append(im)\n",
    "    def save(self):\n",
    "        if self.frames:\n",
    "            self.frames[0].save(self.path, save_all=True, append_images=self.frames[1:],\n",
    "                                optimize=True, duration=self.duration, loop=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2528222c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed eval batch: torch.Size([128, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 1) Data: CIFAR-10 @ 224\n",
    "# =========================\n",
    "BATCH = 128\n",
    "\n",
    "tfm_train = T.Compose([\n",
    "    T.Resize(224),\n",
    "    T.RandomCrop(224, padding=4),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor()\n",
    "])\n",
    "tfm_eval  = T.Compose([\n",
    "    T.Resize(224),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "train_ds = dsets.CIFAR10(root=\"./data\", train=True,  download=True, transform=tfm_train)\n",
    "test_ds  = dsets.CIFAR10(root=\"./data\", train=False, download=True, transform=tfm_eval)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH, shuffle=True,  num_workers=2, drop_last=True)\n",
    "eval_dl  = DataLoader(test_ds,  batch_size=BATCH, shuffle=False, num_workers=2, drop_last=True)\n",
    "\n",
    "fixed_eval_imgs, fixed_eval_labels = next(iter(eval_dl))\n",
    "print(\"Fixed eval batch:\", fixed_eval_imgs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7a80935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid size (patch grid): (14, 14)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 2) Model: ViT tiny 224\n",
    "# =========================\n",
    "model = timm.create_model(\"vit_tiny_patch16_224\", pretrained=False, num_classes=10)\n",
    "model = model.to(DEVICE)\n",
    "print(\"Grid size (patch grid):\", model.patch_embed.grid_size)  # (14, 14)\n",
    "GRID = int(model.patch_embed.grid_size[0])  # should be 14\n",
    "assert GRID == model.patch_embed.grid_size[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ed447af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Drop-in replacement: attention wrapper WITH device/dtype sync & attn_mask support ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from contextlib import contextmanager\n",
    "from timm.models.vision_transformer import Attention\n",
    "\n",
    "class AttentionWithWeights(Attention):\n",
    "    \"\"\"Timm-compatible attention that records probs as (B, Nq, H, Nk) in self.last_attn.\"\"\"\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        B, N, C = x.shape\n",
    "        # qkv: (B, N, 3, heads, head_dim) -> (3, B, heads, N, head_dim)\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)  # each: (B, heads, N, head_dim)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale  # (B, heads, N, N)\n",
    "        if attn_mask is not None:\n",
    "            attn = attn + attn_mask  # broadcast if provided\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        # Store as (B, Nq, H, Nk) to match cls_to_patch_attn / mean_patch_to_patch\n",
    "        self.last_attn = attn.permute(0, 2, 1, 3).detach()  # (B, N, heads, N)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "@contextmanager\n",
    "def patch_vit_attn(vit_model):\n",
    "    \"\"\"\n",
    "    Temporarily replace all blocks[i].attn with AttentionWithWeights,\n",
    "    copying weights & moving to the SAME device/dtype as the original.\n",
    "    \"\"\"\n",
    "    originals = []\n",
    "    try:\n",
    "        for blk in vit_model.blocks:\n",
    "            a: Attention = blk.attn\n",
    "            w = AttentionWithWeights(\n",
    "                dim=a.qkv.in_features,\n",
    "                num_heads=a.num_heads,\n",
    "                qkv_bias=getattr(a, 'qkv_bias', True),\n",
    "                qk_norm=getattr(a, 'qk_norm', False),\n",
    "                attn_drop=getattr(a.attn_drop, 'p', 0.0),\n",
    "                proj_drop=getattr(a.proj_drop, 'p', 0.0),\n",
    "                norm_layer=getattr(a, 'norm_layer', None),\n",
    "                attn_head_dim=getattr(a, 'attn_head_dim', None),\n",
    "            )\n",
    "            # copy state\n",
    "            w.qkv.load_state_dict(a.qkv.state_dict())\n",
    "            w.proj.load_state_dict(a.proj.state_dict())\n",
    "            if hasattr(a, 'q_norm') and hasattr(w, 'q_norm'):\n",
    "                w.q_norm.load_state_dict(a.q_norm.state_dict())\n",
    "            if hasattr(a, 'k_norm') and hasattr(w, 'k_norm'):\n",
    "                w.k_norm.load_state_dict(a.k_norm.state_dict())\n",
    "\n",
    "            # >>> MOVE to SAME device & dtype as original <<<\n",
    "            dev = next(a.parameters()).device\n",
    "            dtype = next(a.parameters()).dtype\n",
    "            w.to(device=dev, dtype=dtype)\n",
    "\n",
    "            originals.append(a)\n",
    "            blk.attn = w\n",
    "        yield\n",
    "    finally:\n",
    "        # restore originals\n",
    "        for blk, a in zip(vit_model.blocks, originals):\n",
    "            blk.attn = a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78a5ad5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 4) Log a structured frame\n",
    "# =========================\n",
    "def build_frame_from_model(step, fixed_imgs, H=128, W=128):\n",
    "    model.eval()\n",
    "    with torch.no_grad(), patch_vit_attn(model):\n",
    "        _ = model(fixed_imgs.to(DEVICE))  # runs with capturing attention\n",
    "\n",
    "        # choose a mid block for visualization (or last if fewer blocks)\n",
    "        blk = model.blocks[3] if len(model.blocks) > 3 else model.blocks[-1]\n",
    "        A = blk.attn.last_attn.detach().cpu()  # (B, Nq, H, Nk)\n",
    "\n",
    "        # Panels:\n",
    "        panelA = np.array(to_img(I don't deal with this like I just I don't deal with this like I just (A, GRID)[0].numpy(), H, W)) / 255.0\n",
    "        panelB = np.array(to_img(mean_patch_to_patch(A)[0].numpy(), H, W)) / 255.0\n",
    "\n",
    "        # ensure token hook fired\n",
    "        if \"tokens\" not in token_outputs:\n",
    "            _ = model(fixed_imgs.to(DEVICE))\n",
    "        toks = token_outputs[\"tokens\"]                     # (B, N, C)\n",
    "        panelC = np.array(to_img(token_gram_from_tokens(toks).cpu().numpy(), H, W)) / 255.0\n",
    "\n",
    "        # stack into HxWx3 frame for ZeroModel, and a grayscale tile grid for preview\n",
    "        frame = np.stack([panelA, panelB, panelC], axis=-1).astype(\"float32\")  # HxWx3\n",
    "        preview = tile([to_img(panelA), to_img(panelB), to_img(panelC)], cols=3)\n",
    "    model.train()\n",
    "    return frame, preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bc4ae22",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but got mat1 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m opt.step()\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m step % LOG_EVERY == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     frame_hwC, panel_preview = \u001b[43mbuild_frame_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_eval_imgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# 1) stream to ZeroModel (H x W x C floats in [0,1])\u001b[39;00m\n\u001b[32m     32\u001b[39m     em.send(step, frame_hwC, tags={\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(loss.item())})\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mbuild_frame_from_model\u001b[39m\u001b[34m(step, fixed_imgs, H, W)\u001b[39m\n\u001b[32m      5\u001b[39m model.eval()\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad(), patch_vit_attn(model):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     _ = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfixed_imgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# runs with capturing attention\u001b[39;00m\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# choose a mid block for visualization (or last if fewer blocks)\u001b[39;00m\n\u001b[32m     10\u001b[39m     blk = model.blocks[\u001b[32m3\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model.blocks) > \u001b[32m3\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m model.blocks[-\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ernan\\Project\\zeromodel\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ernan\\Project\\zeromodel\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ernan\\Project\\zeromodel\\venv\\Lib\\site-packages\\timm\\models\\vision_transformer.py:993\u001b[39m, in \u001b[36mVisionTransformer.forward\u001b[39m\u001b[34m(self, x, attn_mask)\u001b[39m\n\u001b[32m    992\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m993\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    994\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.forward_head(x)\n\u001b[32m    995\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ernan\\Project\\zeromodel\\venv\\Lib\\site-packages\\timm\\models\\vision_transformer.py:948\u001b[39m, in \u001b[36mVisionTransformer.forward_features\u001b[39m\u001b[34m(self, x, attn_mask)\u001b[39m\n\u001b[32m    946\u001b[39m     x = checkpoint_seq(\u001b[38;5;28mself\u001b[39m.blocks, x)\n\u001b[32m    947\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m948\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    950\u001b[39m x = \u001b[38;5;28mself\u001b[39m.norm(x)\n\u001b[32m    951\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ernan\\Project\\zeromodel\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ernan\\Project\\zeromodel\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ernan\\Project\\zeromodel\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ernan\\Project\\zeromodel\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ernan\\Project\\zeromodel\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ernan\\Project\\zeromodel\\venv\\Lib\\site-packages\\timm\\models\\vision_transformer.py:176\u001b[39m, in \u001b[36mBlock.forward\u001b[39m\u001b[34m(self, x, attn_mask)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.drop_path1(\u001b[38;5;28mself\u001b[39m.ls1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[32m    177\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.drop_path2(\u001b[38;5;28mself\u001b[39m.ls2(\u001b[38;5;28mself\u001b[39m.mlp(\u001b[38;5;28mself\u001b[39m.norm2(x))))\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ernan\\Project\\zeromodel\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ernan\\Project\\zeromodel\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mAttentionWithWeights.forward\u001b[39m\u001b[34m(self, x, attn_mask)\u001b[39m\n\u001b[32m     29\u001b[39m B, N, C = x.shape\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# qkv: (B, N, 3, heads, head_dim) -> (3, B, heads, N, head_dim)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m qkv = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mqkv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m.reshape(B, N, \u001b[32m3\u001b[39m, \u001b[38;5;28mself\u001b[39m.num_heads, C // \u001b[38;5;28mself\u001b[39m.num_heads).permute(\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m4\u001b[39m)\n\u001b[32m     32\u001b[39m q, k, v = qkv.unbind(\u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# each: (B, heads, N, head_dim)\u001b[39;00m\n\u001b[32m     34\u001b[39m attn = (q @ k.transpose(-\u001b[32m2\u001b[39m, -\u001b[32m1\u001b[39m)) * \u001b[38;5;28mself\u001b[39m.scale  \u001b[38;5;66;03m# (B, heads, N, N)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ernan\\Project\\zeromodel\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ernan\\Project\\zeromodel\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ernan\\Project\\zeromodel\\venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but got mat1 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 5) Train + Stream to ZeroModel\n",
    "# =========================\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.05)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "gif = GifLogger(\"artifacts/cifar_vit_preview.gif\", fps=8)   # local preview gif\n",
    "em  = ZMFileEmitter(\"artifacts/zm_run1\", fps=8)             # ZeroModel stream\n",
    "\n",
    "LOG_EVERY = 25\n",
    "STEPS = 500\n",
    "\n",
    "model.train()\n",
    "it = iter(train_dl)\n",
    "\n",
    "for step in range(1, STEPS+1):\n",
    "    try:\n",
    "        x, y = next(it)\n",
    "    except StopIteration:\n",
    "        it = iter(train_dl); x, y = next(it)\n",
    "    x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "    opt.zero_grad()\n",
    "    logits = model(x)\n",
    "    loss = loss_fn(logits, y)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if step % LOG_EVERY == 0:\n",
    "        frame_hwC, panel_preview = build_frame_from_model(step, fixed_eval_imgs)\n",
    "        # 1) stream to ZeroModel (H x W x C floats in [0,1])\n",
    "        em.send(step, frame_hwC, tags={\"loss\": float(loss.item())})\n",
    "        # 2) add a tile to local preview GIF\n",
    "        gif.add(panel_preview, text=f\"step {step}  loss {loss.item():.3f}\")\n",
    "\n",
    "em.close()\n",
    "gif.save()\n",
    "print(\"Done. Preview GIF:\", \"artifacts/cifar_vit_preview.gif\")\n",
    "print(\"ZeroModel run dir:\", \"artifacts/zm_run1\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
